# History



* In 1943 Warren McCulloch, a neurophysiologist, and Walter Pitts tried to create a mathematical model to stimulate the human neuron

* Pitss was a genius when he was 12 he was offered the ability to study with the philopsher betrayed russel and turned him done. 

  * Pitts was a pretty strange guy, he refused most jobs and lived most of his life homeless while doing pretty amazing work in mathematics

  

![dlcf_0101](https://learning.oreilly.com/library/view/deep-learning-for/9781492045519/assets/dlcf_0101.png)

* There work was continued by a physiologist named Frank Rosenblat and he created a new model called the Mark I Perceptron. 



![Frank Rosenblatt with a Mark I Perceptron computer in 1960 : EngineeringPorn](https://preview.redd.it/bo3xoa8y4m341.jpg?auto=webp&s=8417f35c35c9b31541fa8643eced0038aba87c4f)

![img](https://upload.wikimedia.org/wikipedia/commons/thumb/8/8a/Perceptron_example.svg/500px-Perceptron_example.svg.png)

A MIT professor showed a flaw of the perceptron however. He said for some functions such as the popular xor that a single perceptron would be unable to solve. 

![The exclusive-or (XOR) function is a nonlinear function that returns 0... |  Download Scientific Diagram](https://www.researchgate.net/profile/Brian_Spears/publication/322048911/figure/fig4/AS:667820685807618@1536232267250/The-exclusive-or-XOR-function-is-a-nonlinear-function-that-returns-0-when-its-two.png)



* However, it can be solve in with multiple ones in a layer. 
  * But what if we added a 2nd one, 



However, people weren't paying attention to this so an AI winter began. For 20 or 30 years or so no big advances were made . Progress kinda stalled and people believed it was all hype



![Image for post](https://miro.medium.com/max/1400/1*mWYZanOv3QUafz0nnhbEWw.png)

## The Deep Learning Revolution



## What caused Deep Learning to sky rocket in popularity?



* Concrete Developments
  * December 2012 at NIPS conference 
    * " [Geoff Hinton](https://en.wikipedia.org/wiki/Geoffrey_Hinton) and two of his graduate students showed you could take a very large dataset called ImageNet, with 10,000 categories and 10 million images, and reduce the classification error by 20 percent using deep learning." 
  * Convulational Neural Networks which is a multilayer perception with some optimizations is shown to be quite good 
* Gamers
  * Gamers began to use GPUs pretty regularly
  * GPU is a graphical processing unit
  * GPU's more efficiently handle data in parallel and have optimizations to allow them 
  * Nvidia, the biggest manufacture, releases CUDA which is one of the first really easy ways to code for a GPU
* The Growth of the Cloud
  * In 2010, Amazon Web Services allows their customers to rent GPUs.
    * This limits the barrier to entry. Now, I don't have to own an 3000 GPU, I can rent one for $1 an hour.\





### 

![gpu_cpu_comparison](https://course19.fast.ai/images/gpu/gpu_cpu_comparison.png)





### Mini Activity 1

Create a classifier on https://teachablemachine.withgoogle.com/



### Mini Activity 2

Each team research something I believe will be important/ cool in the coming year or two with deep learning

* GovCloud
* Data Augmentation
* Style Transfer
* Deoldify
* AR Copy Paste



## Sources

https://towardsdatascience.com/history-of-the-second-ai-winter-406f18789d45

https://medium.com/@nazanindelam/single-layer-artificial-neural-networks-a91cf3752a86

https://www.theverge.com/2018/10/16/17985168/deep-learning-revolution-terrence-sejnowski-artificial-intelligence-technology

https://en.wikipedia.org/wiki/Graphics_processing_unit

https://course19.fast.ai/gpu_tutorial.html

https://course.fast.ai/